{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_tweets import get_data, give_emoji_free_text, CustomDataset\n",
    "\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=\"../data/train_en.tsv\"\n",
    "test_file=\"../data/dev_en.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file, sep='\\t',skiprows=0, encoding = 'utf-8')\n",
    "train_df['clean text'] = train_df['text'].apply(give_emoji_free_text)\n",
    "test_df = pd.read_csv(test_file, sep='\\t',skiprows=0, encoding = 'utf-8')\n",
    "test_df['clean text'] = test_df['text'].apply(give_emoji_free_text)\n",
    "\n",
    "_, _, _, longest_sent, _, _ = get_data(train_file, test_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional pip installs I had to run:\n",
    "pip install emoji\n",
    "pip install nltk\n",
    "pip install transformers[torch]\n",
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert-specific tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe --> Dataset\n",
    "hate_train_dataset = CustomDataset(longest_sent=longest_sent, data=train_df, tokenizer=tokenizer)\n",
    "hate_test_dataset = CustomDataset(longest_sent=longest_sent, data=test_df, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hate_train_dataloader = DataLoader(hate_train_dataset, batch_size=10)\n",
    "hate_test_dataloader = DataLoader(hate_test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR decay schedule\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(hate_train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "bert_train_acc_epoch = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in hate_train_dataloader:\n",
    "        # print(batch)\n",
    "        # break\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
