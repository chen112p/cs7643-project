{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_tweets import get_data\n",
    "from rnn import MyModel\n",
    "from train_validate import (train, validate, convert_tweet2tensor)\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate vanilla LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "train_batch_size = 250\n",
    "test_batch_size = 250\n",
    "embedding_size = 12\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "num_classes = 2\n",
    "bidrectional = True\n",
    "EPOCH = 20\n",
    "train_file=\"../data/train_en.tsv\"\n",
    "test_file=\"../data/dev_en.tsv\"\n",
    "device=\"cpu\"\n",
    "\n",
    "train_dataset, test_dataset, alphabet, longest_sent, train_x, test_x = get_data(train_file, test_file)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "model = MyModel(\n",
    "    len(alphabet),\n",
    "    longest_sent,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    num_layers,\n",
    "    dropout,\n",
    "    num_classes,\n",
    "    bidrectional,\n",
    "    device\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "## training loop\n",
    "best = 0.0\n",
    "best_cm = None\n",
    "best_model = None\n",
    "train_acc_epoch = []\n",
    "valid_acc_epoch = []\n",
    "for epoch in range(EPOCH):\n",
    "    # train loop\n",
    "    train_acc, train_cm = train(epoch, train_loader, model, optimizer, criterion)\n",
    "    train_acc_epoch.append(train_acc.detach().cpu().numpy())\n",
    "\n",
    "    # validation loop\n",
    "    valid_acc, valid_cm = validate(epoch, test_loader, model, criterion)\n",
    "    valid_acc_epoch.append(valid_acc.detach().cpu().numpy())\n",
    "\n",
    "    if valid_acc > best:\n",
    "        best = valid_acc\n",
    "        best_cm = valid_cm\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "print('Best Prec @1 Acccuracy: {:.4f}'.format(best))\n",
    "per_cls_acc = best_cm.diag().detach().numpy().tolist()\n",
    "for i, acc_i in enumerate(per_cls_acc):\n",
    "    print(\"Accuracy of Class {}: {:.4f}\".format(i, acc_i))\n",
    "\n",
    "# save best model\n",
    "pickle.dump(best_model, open(\"best_RNN_model.pkl\", 'wb'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(EPOCH), train_acc_epoch, label='train')\n",
    "plt.plot(range(EPOCH), valid_acc_epoch, label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"accuracy curve\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open(\"best_RNN_model.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"best_RNN_model.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_file, sep='\\t',skiprows=0, encoding = 'utf-8')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_df.sample(10, random_state=0)\n",
    "for idx, row in test_sample.iterrows():\n",
    "    test_tensor = convert_tweet2tensor(row['text'], alphabet, longest_sent)\n",
    "    print(f\"test text {test_df.loc[idx,'text']}\")\n",
    "    pred_prob = loaded_model.forward(test_tensor)\n",
    "    pred_label = torch.argmax(pred_prob)\n",
    "    print(f\"true label {test_df.loc[idx,'HS']}\")\n",
    "    print(f\"best model predicted label {pred_label}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03af33321e8c50997747d06c5c21eb90732c28b7dbbbf890c48fc1c524afe0f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('tweets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
